{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: Removing pixels with grayscalse intensity $<250$\n",
    "Model: 4 layers CNN\n",
    "Implementation of early stopping\n",
    "# New: Tuning of learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/perreaultlafleur/anaconda3/lib/python3.5/site-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#IMPORT LIBRARIES\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "import time\n",
    "import datetime as dt\n",
    "import progressbar\n",
    "\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "#from pytorchtools import EarlyStopping\n",
    "#import torchvision\n",
    "#import torchvision.transforms\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#IMPORT DATA\n",
    "train_images = pd.read_pickle('train_max_x')\n",
    "train_answers = pd.read_csv('train_max_y.csv')\n",
    "test_images = pd.read_pickle('test_max_x')\n",
    "\n",
    "#Define training, testing and validation datasets\n",
    "X_train = train_images[0:45000]; X_test=train_images[45000:50000]; X_valid=test_images\n",
    "y_train = train_answers.loc[0:44999,'Label']; y_test = train_answers.loc[45000:49999,'Label']\n",
    "\n",
    "#Take out the funky backgrounds\n",
    "X_train = (X_train>=250).astype(float)*255\n",
    "X_test = (X_test>=250).astype(float)*255\n",
    "X_valid = (X_valid>=250).astype(float)*255\n",
    "\n",
    "#Add one dimension so that it can be an input for the NN\n",
    "X_train = X_train.reshape(45000, 1, 128, 128)\n",
    "X_test = X_test.reshape(5000, 1, 128, 128)\n",
    "X_valid = X_valid.reshape(10000, 1, 128, 128)\n",
    "\n",
    "#Change the type from numpy to tensor so that it can be an input for the NN\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "X_valid = torch.from_numpy(X_valid)\n",
    "\n",
    "#do the same for the target vectors\n",
    "y_train = torch.from_numpy(y_train.to_numpy().reshape(45000))\n",
    "y_test = torch.from_numpy(y_test.to_numpy().reshape(5000))\n",
    "\n",
    "#join the x and y values\n",
    "train = data_utils.TensorDataset(X_train, y_train)\n",
    "test = data_utils.TensorDataset(X_test, y_test)\n",
    "valid = data_utils.TensorDataset(X_valid)\n",
    "\n",
    "#create an iterator that will split the data in batches for training\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=256)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=256)\n",
    "validloader = torch.utils.data.DataLoader(X_valid, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"Convnet Classifier\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "            # Layer 4\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        )\n",
    "        # Logistic Regression\n",
    "        self.clf = nn.Linear(8192, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(self.conv(x).squeeze().reshape(64,-1).shape)\n",
    "        out = self.conv(x)\n",
    "        return self.clf(out.reshape(out.size(0), -1))\n",
    "        #return self.clf(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch : 0 Loss : 8.247 \n",
      "Epoch : 0 Test Acc : 20.300\n",
      "Epoch : 0 Test Loss : 2.127 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 20.299999237060547 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 1 Loss : 3.049 \n",
      "Epoch : 1 Test Acc : 20.960\n",
      "Epoch : 1 Test Loss : 2.210 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.6599998474121094 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 2 Loss : 2.272 \n",
      "Epoch : 2 Test Acc : 23.060\n",
      "Epoch : 2 Test Loss : 2.233 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 2.1000003814697266 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 3 Loss : 2.075 \n",
      "Epoch : 3 Test Acc : 24.640\n",
      "Epoch : 3 Test Loss : 2.239 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.5799999237060547 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 4 Loss : 2.009 \n",
      "Epoch : 4 Test Acc : 25.320\n",
      "Epoch : 4 Test Loss : 2.236 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.6800003051757812 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 5 Loss : 1.973 \n",
      "Epoch : 5 Test Acc : 26.340\n",
      "Epoch : 5 Test Loss : 2.230 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.0200004577636719 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 6 Loss : 1.955 \n",
      "Epoch : 6 Test Acc : 26.600\n",
      "Epoch : 6 Test Loss : 2.220 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.26000022888183594 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 7 Loss : 1.941 \n",
      "Epoch : 7 Test Acc : 26.600\n",
      "Epoch : 7 Test Loss : 2.210 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 8 Loss : 1.926 \n",
      "Epoch : 8 Test Acc : 26.840\n",
      "Epoch : 8 Test Loss : 2.197 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.23999977111816406 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 9 Loss : 1.917 \n",
      "Epoch : 9 Test Acc : 27.000\n",
      "Epoch : 9 Test Loss : 2.187 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.15999984741210938 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 10 Loss : 1.908 \n",
      "Epoch : 10 Test Acc : 27.180\n",
      "Epoch : 10 Test Loss : 2.175 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.18000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 11 Loss : 1.901 \n",
      "Epoch : 11 Test Acc : 27.220\n",
      "Epoch : 11 Test Loss : 2.165 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.03999900817871094 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 12 Loss : 1.895 \n",
      "Epoch : 12 Test Acc : 27.220\n",
      "Epoch : 12 Test Loss : 2.157 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 13 Loss : 1.892 \n",
      "Epoch : 13 Test Acc : 27.220\n",
      "Epoch : 13 Test Loss : 2.153 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 14 Loss : 1.890 \n",
      "Epoch : 14 Test Acc : 27.220\n",
      "Epoch : 14 Test Loss : 2.147 \n",
      "Iterator for early stopping now at 4 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 15 Loss : 1.888 \n",
      "Epoch : 15 Test Acc : 27.220\n",
      "Epoch : 15 Test Loss : 2.145 \n",
      "Iterator for early stopping now at 5 since improvement of test accuracy = 0.0 <= 0.1\n",
      "Early stopping\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 16 Loss : 1.886 \n",
      "Epoch : 16 Test Acc : 27.220\n",
      "Epoch : 16 Test Loss : 2.141 \n",
      "Iterator for early stopping now at 6 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 17 Loss : 1.886 \n",
      "Epoch : 17 Test Acc : 27.220\n",
      "Epoch : 17 Test Loss : 2.138 \n",
      "Iterator for early stopping now at 7 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 18 Loss : 1.884 \n",
      "Epoch : 18 Test Acc : 27.220\n",
      "Epoch : 18 Test Loss : 2.136 \n",
      "Iterator for early stopping now at 8 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 19 Loss : 1.884 \n",
      "Epoch : 19 Test Acc : 27.220\n",
      "Epoch : 19 Test Loss : 2.134 \n",
      "Iterator for early stopping now at 9 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 20 Loss : 1.882 \n",
      "Epoch : 20 Test Acc : 27.220\n",
      "Epoch : 20 Test Loss : 2.130 \n",
      "Iterator for early stopping now at 10 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 21 Loss : 1.882 \n",
      "Epoch : 21 Test Acc : 27.220\n",
      "Epoch : 21 Test Loss : 2.128 \n",
      "Iterator for early stopping now at 11 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 22 Loss : 1.880 \n",
      "Epoch : 22 Test Acc : 27.220\n",
      "Epoch : 22 Test Loss : 2.123 \n",
      "Iterator for early stopping now at 12 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 23 Loss : 1.878 \n",
      "Epoch : 23 Test Acc : 27.220\n",
      "Epoch : 23 Test Loss : 2.118 \n",
      "Iterator for early stopping now at 13 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 24 Loss : 1.873 \n",
      "Epoch : 24 Test Acc : 27.360\n",
      "Epoch : 24 Test Loss : 2.110 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.1400012969970703 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 25 Loss : 1.867 \n",
      "Epoch : 25 Test Acc : 28.020\n",
      "Epoch : 25 Test Loss : 2.103 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.6599998474121094 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 26 Loss : 1.859 \n",
      "Epoch : 26 Test Acc : 29.240\n",
      "Epoch : 26 Test Loss : 2.094 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.2199993133544922 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 27 Loss : 1.850 \n",
      "Epoch : 27 Test Acc : 29.360\n",
      "Epoch : 27 Test Loss : 2.087 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.12000083923339844 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 28 Loss : 1.841 \n",
      "Epoch : 28 Test Acc : 29.860\n",
      "Epoch : 28 Test Loss : 2.077 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.5 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 29 Loss : 1.828 \n",
      "Epoch : 29 Test Acc : 30.920\n",
      "Epoch : 29 Test Loss : 2.068 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.0599994659423828 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 30 Loss : 1.817 \n",
      "Epoch : 30 Test Acc : 31.580\n",
      "Epoch : 30 Test Loss : 2.057 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.6599998474121094 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 31 Loss : 1.800 \n",
      "Epoch : 31 Test Acc : 33.280\n",
      "Epoch : 31 Test Loss : 2.041 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.6999988555908203 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 32 Loss : 1.786 \n",
      "Epoch : 32 Test Acc : 34.760\n",
      "Epoch : 32 Test Loss : 2.025 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.4799995422363281 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 33 Loss : 1.767 \n",
      "Epoch : 33 Test Acc : 36.100\n",
      "Epoch : 33 Test Loss : 2.001 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.3400001525878906 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 34 Loss : 1.739 \n",
      "Epoch : 34 Test Acc : 37.320\n",
      "Epoch : 34 Test Loss : 1.978 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.220001220703125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 35 Loss : 1.714 \n",
      "Epoch : 35 Test Acc : 37.800\n",
      "Epoch : 35 Test Loss : 1.949 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.4799995422363281 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 36 Loss : 1.691 \n",
      "Epoch : 36 Test Acc : 39.220\n",
      "Epoch : 36 Test Loss : 1.928 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.4200019836425781 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 37 Loss : 1.675 \n",
      "Epoch : 37 Test Acc : 40.060\n",
      "Epoch : 37 Test Loss : 1.906 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.8400001525878906 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 38 Loss : 1.648 \n",
      "Epoch : 38 Test Acc : 41.540\n",
      "Epoch : 38 Test Loss : 1.880 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.4799995422363281 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 39 Loss : 1.622 \n",
      "Epoch : 39 Test Acc : 42.800\n",
      "Epoch : 39 Test Loss : 1.859 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.2599983215332031 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 40 Loss : 1.601 \n",
      "Epoch : 40 Test Acc : 43.420\n",
      "Epoch : 40 Test Loss : 1.844 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.6199989318847656 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 41 Loss : 1.576 \n",
      "Epoch : 41 Test Acc : 43.840\n",
      "Epoch : 41 Test Loss : 1.817 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.4200019836425781 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 42 Loss : 1.561 \n",
      "Epoch : 42 Test Acc : 44.700\n",
      "Epoch : 42 Test Loss : 1.804 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.8600006103515625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 43 Loss : 1.546 \n",
      "Epoch : 43 Test Acc : 45.660\n",
      "Epoch : 43 Test Loss : 1.793 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.9599990844726562 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 44 Loss : 1.522 \n",
      "Epoch : 44 Test Acc : 45.800\n",
      "Epoch : 44 Test Loss : 1.766 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.1399993896484375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 45 Loss : 1.506 \n",
      "Epoch : 45 Test Acc : 47.060\n",
      "Epoch : 45 Test Loss : 1.760 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.2600021362304688 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 46 Loss : 1.485 \n",
      "Epoch : 46 Test Acc : 48.660\n",
      "Epoch : 46 Test Loss : 1.745 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.5999984741210938 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 47 Loss : 1.468 \n",
      "Epoch : 47 Test Acc : 48.080\n",
      "Epoch : 47 Test Loss : 1.718 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.5799980163574219 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 48 Loss : 1.453 \n",
      "Epoch : 48 Test Acc : 49.360\n",
      "Epoch : 48 Test Loss : 1.705 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.279998779296875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 49 Loss : 1.439 \n",
      "Epoch : 49 Test Acc : 50.820\n",
      "Epoch : 49 Test Loss : 1.699 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.4599990844726562 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 50 Loss : 1.420 \n",
      "Epoch : 50 Test Acc : 51.300\n",
      "Epoch : 50 Test Loss : 1.689 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.4799995422363281 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 51 Loss : 1.404 \n",
      "Epoch : 51 Test Acc : 51.380\n",
      "Epoch : 51 Test Loss : 1.661 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 52 Loss : 1.384 \n",
      "Epoch : 52 Test Acc : 52.720\n",
      "Epoch : 52 Test Loss : 1.651 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.3400001525878906 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 53 Loss : 1.370 \n",
      "Epoch : 53 Test Acc : 53.240\n",
      "Epoch : 53 Test Loss : 1.634 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.5200004577636719 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 54 Loss : 1.354 \n",
      "Epoch : 54 Test Acc : 54.800\n",
      "Epoch : 54 Test Loss : 1.628 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.55999755859375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 55 Loss : 1.333 \n",
      "Epoch : 55 Test Acc : 54.140\n",
      "Epoch : 55 Test Loss : 1.601 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.6599998474121094 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 56 Loss : 1.319 \n",
      "Epoch : 56 Test Acc : 56.240\n",
      "Epoch : 56 Test Loss : 1.591 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 2.1000022888183594 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 57 Loss : 1.306 \n",
      "Epoch : 57 Test Acc : 57.080\n",
      "Epoch : 57 Test Loss : 1.577 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.8400001525878906 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 58 Loss : 1.282 \n",
      "Epoch : 58 Test Acc : 57.300\n",
      "Epoch : 58 Test Loss : 1.552 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.21999740600585938 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 59 Loss : 1.270 \n",
      "Epoch : 59 Test Acc : 58.420\n",
      "Epoch : 59 Test Loss : 1.548 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.1199989318847656 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 60 Loss : 1.254 \n",
      "Epoch : 60 Test Acc : 60.840\n",
      "Epoch : 60 Test Loss : 1.534 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 2.420001983642578 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 61 Loss : 1.231 \n",
      "Epoch : 61 Test Acc : 61.260\n",
      "Epoch : 61 Test Loss : 1.513 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.4199981689453125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 62 Loss : 1.211 \n",
      "Epoch : 62 Test Acc : 62.000\n",
      "Epoch : 62 Test Loss : 1.494 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.7400016784667969 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 63 Loss : 1.198 \n",
      "Epoch : 63 Test Acc : 63.300\n",
      "Epoch : 63 Test Loss : 1.476 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.2999992370605469 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 64 Loss : 1.175 \n",
      "Epoch : 64 Test Acc : 65.080\n",
      "Epoch : 64 Test Loss : 1.464 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.7800025939941406 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 65 Loss : 1.161 \n",
      "Epoch : 65 Test Acc : 65.420\n",
      "Epoch : 65 Test Loss : 1.441 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.339996337890625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 66 Loss : 1.146 \n",
      "Epoch : 66 Test Acc : 66.680\n",
      "Epoch : 66 Test Loss : 1.433 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.2600021362304688 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 67 Loss : 1.133 \n",
      "Epoch : 67 Test Acc : 66.760\n",
      "Epoch : 67 Test Loss : 1.415 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 68 Loss : 1.111 \n",
      "Epoch : 68 Test Acc : 67.380\n",
      "Epoch : 68 Test Loss : 1.395 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.6199951171875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 69 Loss : 1.099 \n",
      "Epoch : 69 Test Acc : 68.520\n",
      "Epoch : 69 Test Loss : 1.381 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.1399993896484375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 70 Loss : 1.087 \n",
      "Epoch : 70 Test Acc : 69.400\n",
      "Epoch : 70 Test Loss : 1.374 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.8800048828125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 71 Loss : 1.073 \n",
      "Epoch : 71 Test Acc : 69.980\n",
      "Epoch : 71 Test Loss : 1.360 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.5800018310546875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 72 Loss : 1.055 \n",
      "Epoch : 72 Test Acc : 70.040\n",
      "Epoch : 72 Test Loss : 1.341 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.05999755859375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 73 Loss : 1.045 \n",
      "Epoch : 73 Test Acc : 69.800\n",
      "Epoch : 73 Test Loss : 1.322 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.23999786376953125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 74 Loss : 1.032 \n",
      "Epoch : 74 Test Acc : 71.020\n",
      "Epoch : 74 Test Loss : 1.321 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.2199935913085938 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 75 Loss : 1.017 \n",
      "Epoch : 75 Test Acc : 71.180\n",
      "Epoch : 75 Test Loss : 1.304 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.160003662109375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 76 Loss : 1.007 \n",
      "Epoch : 76 Test Acc : 72.160\n",
      "Epoch : 76 Test Loss : 1.293 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.9800033569335938 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 77 Loss : 0.996 \n",
      "Epoch : 77 Test Acc : 72.560\n",
      "Epoch : 77 Test Loss : 1.288 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.399993896484375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 78 Loss : 0.980 \n",
      "Epoch : 78 Test Acc : 73.100\n",
      "Epoch : 78 Test Loss : 1.274 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.5400009155273438 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 79 Loss : 0.966 \n",
      "Epoch : 79 Test Acc : 73.300\n",
      "Epoch : 79 Test Loss : 1.260 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.20000457763671875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 80 Loss : 0.959 \n",
      "Epoch : 80 Test Acc : 73.480\n",
      "Epoch : 80 Test Loss : 1.257 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.18000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 81 Loss : 0.945 \n",
      "Epoch : 81 Test Acc : 74.220\n",
      "Epoch : 81 Test Loss : 1.237 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.7399978637695312 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 82 Loss : 0.934 \n",
      "Epoch : 82 Test Acc : 74.880\n",
      "Epoch : 82 Test Loss : 1.232 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.6599960327148438 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 83 Loss : 0.925 \n",
      "Epoch : 83 Test Acc : 74.960\n",
      "Epoch : 83 Test Loss : 1.219 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 84 Loss : 0.914 \n",
      "Epoch : 84 Test Acc : 75.000\n",
      "Epoch : 84 Test Loss : 1.205 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = 0.04000091552734375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 85 Loss : 0.905 \n",
      "Epoch : 85 Test Acc : 75.700\n",
      "Epoch : 85 Test Loss : 1.201 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.6999969482421875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 86 Loss : 0.892 \n",
      "Epoch : 86 Test Acc : 76.060\n",
      "Epoch : 86 Test Loss : 1.187 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.3600006103515625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 87 Loss : 0.878 \n",
      "Epoch : 87 Test Acc : 76.660\n",
      "Epoch : 87 Test Loss : 1.180 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.600006103515625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 88 Loss : 0.872 \n",
      "Epoch : 88 Test Acc : 76.920\n",
      "Epoch : 88 Test Loss : 1.175 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.2599945068359375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 89 Loss : 0.858 \n",
      "Epoch : 89 Test Acc : 77.740\n",
      "Epoch : 89 Test Loss : 1.161 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.8199996948242188 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 90 Loss : 0.858 \n",
      "Epoch : 90 Test Acc : 77.680\n",
      "Epoch : 90 Test Loss : 1.160 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.05999755859375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 91 Loss : 0.844 \n",
      "Epoch : 91 Test Acc : 77.780\n",
      "Epoch : 91 Test Loss : 1.144 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = 0.09999847412109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 92 Loss : 0.846 \n",
      "Epoch : 92 Test Acc : 78.280\n",
      "Epoch : 92 Test Loss : 1.146 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.5 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 93 Loss : 0.828 \n",
      "Epoch : 93 Test Acc : 78.800\n",
      "Epoch : 93 Test Loss : 1.141 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.5200042724609375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 94 Loss : 0.823 \n",
      "Epoch : 94 Test Acc : 78.980\n",
      "Epoch : 94 Test Loss : 1.122 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.18000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 95 Loss : 0.813 \n",
      "Epoch : 95 Test Acc : 78.920\n",
      "Epoch : 95 Test Loss : 1.114 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.06000518798828125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 96 Loss : 0.806 \n",
      "Epoch : 96 Test Acc : 79.000\n",
      "Epoch : 96 Test Loss : 1.104 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = 0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 97 Loss : 0.791 \n",
      "Epoch : 97 Test Acc : 79.700\n",
      "Epoch : 97 Test Loss : 1.097 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.6999969482421875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 98 Loss : 0.785 \n",
      "Epoch : 98 Test Acc : 79.680\n",
      "Epoch : 98 Test Loss : 1.095 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.01999664306640625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 99 Loss : 0.775 \n",
      "Epoch : 99 Test Acc : 79.840\n",
      "Epoch : 99 Test Loss : 1.080 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.15999603271484375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 100 Loss : 0.776 \n",
      "Epoch : 100 Test Acc : 80.700\n",
      "Epoch : 100 Test Loss : 1.073 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.8600006103515625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 101 Loss : 0.765 \n",
      "Epoch : 101 Test Acc : 80.260\n",
      "Epoch : 101 Test Loss : 1.065 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.43999481201171875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 102 Loss : 0.753 \n",
      "Epoch : 102 Test Acc : 81.300\n",
      "Epoch : 102 Test Loss : 1.062 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.0400009155273438 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 103 Loss : 0.747 \n",
      "Epoch : 103 Test Acc : 81.120\n",
      "Epoch : 103 Test Loss : 1.051 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.18000030517578125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 104 Loss : 0.738 \n",
      "Epoch : 104 Test Acc : 81.140\n",
      "Epoch : 104 Test Loss : 1.044 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = 0.01999664306640625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 105 Loss : 0.733 \n",
      "Epoch : 105 Test Acc : 81.660\n",
      "Epoch : 105 Test Loss : 1.044 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.5200042724609375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 106 Loss : 0.726 \n",
      "Epoch : 106 Test Acc : 81.600\n",
      "Epoch : 106 Test Loss : 1.030 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.06000518798828125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 107 Loss : 0.728 \n",
      "Epoch : 107 Test Acc : 81.580\n",
      "Epoch : 107 Test Loss : 1.034 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.01999664306640625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 108 Loss : 0.713 \n",
      "Epoch : 108 Test Acc : 82.140\n",
      "Epoch : 108 Test Loss : 1.023 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.55999755859375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 109 Loss : 0.715 \n",
      "Epoch : 109 Test Acc : 82.020\n",
      "Epoch : 109 Test Loss : 1.021 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.12000274658203125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 110 Loss : 0.709 \n",
      "Epoch : 110 Test Acc : 81.940\n",
      "Epoch : 110 Test Loss : 1.013 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.07999420166015625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 111 Loss : 0.700 \n",
      "Epoch : 111 Test Acc : 82.400\n",
      "Epoch : 111 Test Loss : 1.000 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.45999908447265625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 112 Loss : 0.690 \n",
      "Epoch : 112 Test Acc : 82.020\n",
      "Epoch : 112 Test Loss : 0.999 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.3800048828125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 113 Loss : 0.693 \n",
      "Epoch : 113 Test Acc : 82.420\n",
      "Epoch : 113 Test Loss : 1.001 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.40000152587890625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 114 Loss : 0.674 \n",
      "Epoch : 114 Test Acc : 82.440\n",
      "Epoch : 114 Test Loss : 0.985 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.0200042724609375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 115 Loss : 0.676 \n",
      "Epoch : 115 Test Acc : 82.620\n",
      "Epoch : 115 Test Loss : 0.984 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.18000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 116 Loss : 0.672 \n",
      "Epoch : 116 Test Acc : 82.840\n",
      "Epoch : 116 Test Loss : 0.984 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.21999359130859375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 117 Loss : 0.670 \n",
      "Epoch : 117 Test Acc : 83.140\n",
      "Epoch : 117 Test Loss : 0.976 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.3000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 118 Loss : 0.661 \n",
      "Epoch : 118 Test Acc : 82.900\n",
      "Epoch : 118 Test Loss : 0.966 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.23999786376953125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 119 Loss : 0.650 \n",
      "Epoch : 119 Test Acc : 83.060\n",
      "Epoch : 119 Test Loss : 0.951 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.15999603271484375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 120 Loss : 0.657 \n",
      "Epoch : 120 Test Acc : 83.740\n",
      "Epoch : 120 Test Loss : 0.966 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.6800003051757812 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 121 Loss : 0.650 \n",
      "Epoch : 121 Test Acc : 83.600\n",
      "Epoch : 121 Test Loss : 0.964 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.1399993896484375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 122 Loss : 0.639 \n",
      "Epoch : 122 Test Acc : 83.440\n",
      "Epoch : 122 Test Loss : 0.945 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.15999603271484375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 123 Loss : 0.642 \n",
      "Epoch : 123 Test Acc : 83.920\n",
      "Epoch : 123 Test Loss : 0.950 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.4799957275390625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 124 Loss : 0.635 \n",
      "Epoch : 124 Test Acc : 84.180\n",
      "Epoch : 124 Test Loss : 0.946 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.26000213623046875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 125 Loss : 0.627 \n",
      "Epoch : 125 Test Acc : 84.100\n",
      "Epoch : 125 Test Loss : 0.941 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 126 Loss : 0.623 \n",
      "Epoch : 126 Test Acc : 84.280\n",
      "Epoch : 126 Test Loss : 0.929 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.18000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 127 Loss : 0.622 \n",
      "Epoch : 127 Test Acc : 84.220\n",
      "Epoch : 127 Test Loss : 0.934 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.05999755859375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 128 Loss : 0.616 \n",
      "Epoch : 128 Test Acc : 84.520\n",
      "Epoch : 128 Test Loss : 0.923 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.29999542236328125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 129 Loss : 0.618 \n",
      "Epoch : 129 Test Acc : 84.240\n",
      "Epoch : 129 Test Loss : 0.924 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.279998779296875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 130 Loss : 0.601 \n",
      "Epoch : 130 Test Acc : 84.560\n",
      "Epoch : 130 Test Loss : 0.918 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.31999969482421875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 131 Loss : 0.601 \n",
      "Epoch : 131 Test Acc : 85.020\n",
      "Epoch : 131 Test Loss : 0.918 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.45999908447265625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 132 Loss : 0.598 \n",
      "Epoch : 132 Test Acc : 84.820\n",
      "Epoch : 132 Test Loss : 0.909 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.1999969482421875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 133 Loss : 0.594 \n",
      "Epoch : 133 Test Acc : 84.980\n",
      "Epoch : 133 Test Loss : 0.910 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.160003662109375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 134 Loss : 0.593 \n",
      "Epoch : 134 Test Acc : 85.160\n",
      "Epoch : 134 Test Loss : 0.902 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.18000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 135 Loss : 0.590 \n",
      "Epoch : 135 Test Acc : 85.120\n",
      "Epoch : 135 Test Loss : 0.903 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.04000091552734375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 136 Loss : 0.587 \n",
      "Epoch : 136 Test Acc : 84.860\n",
      "Epoch : 136 Test Loss : 0.895 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.26000213623046875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 137 Loss : 0.588 \n",
      "Epoch : 137 Test Acc : 85.240\n",
      "Epoch : 137 Test Loss : 0.899 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.37999725341796875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 138 Loss : 0.578 \n",
      "Epoch : 138 Test Acc : 85.360\n",
      "Epoch : 138 Test Loss : 0.897 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.12000274658203125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 139 Loss : 0.576 \n",
      "Epoch : 139 Test Acc : 85.260\n",
      "Epoch : 139 Test Loss : 0.889 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.09999847412109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 140 Loss : 0.575 \n",
      "Epoch : 140 Test Acc : 84.880\n",
      "Epoch : 140 Test Loss : 0.887 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.3800048828125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 141 Loss : 0.571 \n",
      "Epoch : 141 Test Acc : 85.620\n",
      "Epoch : 141 Test Loss : 0.890 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.7400054931640625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 142 Loss : 0.565 \n",
      "Epoch : 142 Test Acc : 85.500\n",
      "Epoch : 142 Test Loss : 0.881 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.12000274658203125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 143 Loss : 0.559 \n",
      "Epoch : 143 Test Acc : 85.360\n",
      "Epoch : 143 Test Loss : 0.872 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.1399993896484375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 144 Loss : 0.560 \n",
      "Epoch : 144 Test Acc : 85.760\n",
      "Epoch : 144 Test Loss : 0.871 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.40000152587890625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 145 Loss : 0.556 \n",
      "Epoch : 145 Test Acc : 85.820\n",
      "Epoch : 145 Test Loss : 0.870 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.05999755859375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 146 Loss : 0.553 \n",
      "Epoch : 146 Test Acc : 85.560\n",
      "Epoch : 146 Test Loss : 0.868 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.26000213623046875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 147 Loss : 0.554 \n",
      "Epoch : 147 Test Acc : 85.600\n",
      "Epoch : 147 Test Loss : 0.865 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = 0.04000091552734375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 148 Loss : 0.549 \n",
      "Epoch : 148 Test Acc : 85.900\n",
      "Epoch : 148 Test Loss : 0.866 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.3000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 149 Loss : 0.538 \n",
      "Epoch : 149 Test Acc : 85.480\n",
      "Epoch : 149 Test Loss : 0.858 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.4199981689453125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 150 Loss : 0.540 \n",
      "Epoch : 150 Test Acc : 85.860\n",
      "Epoch : 150 Test Loss : 0.860 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.37999725341796875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 151 Loss : 0.543 \n",
      "Epoch : 151 Test Acc : 85.520\n",
      "Epoch : 151 Test Loss : 0.857 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.34000396728515625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 152 Loss : 0.536 \n",
      "Epoch : 152 Test Acc : 85.960\n",
      "Epoch : 152 Test Loss : 0.853 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.44000244140625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 153 Loss : 0.528 \n",
      "Epoch : 153 Test Acc : 85.840\n",
      "Epoch : 153 Test Loss : 0.851 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.12000274658203125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 154 Loss : 0.533 \n",
      "Epoch : 154 Test Acc : 86.260\n",
      "Epoch : 154 Test Loss : 0.847 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.42000579833984375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 155 Loss : 0.531 \n",
      "Epoch : 155 Test Acc : 86.460\n",
      "Epoch : 155 Test Loss : 0.840 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.1999969482421875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 156 Loss : 0.522 \n",
      "Epoch : 156 Test Acc : 86.460\n",
      "Epoch : 156 Test Loss : 0.845 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 157 Loss : 0.527 \n",
      "Epoch : 157 Test Acc : 86.340\n",
      "Epoch : 157 Test Loss : 0.844 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.12000274658203125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 158 Loss : 0.517 \n",
      "Epoch : 158 Test Acc : 86.260\n",
      "Epoch : 158 Test Loss : 0.835 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = -0.07999420166015625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 159 Loss : 0.524 \n",
      "Epoch : 159 Test Acc : 86.400\n",
      "Epoch : 159 Test Loss : 0.837 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.1399993896484375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 160 Loss : 0.520 \n",
      "Epoch : 160 Test Acc : 86.080\n",
      "Epoch : 160 Test Loss : 0.839 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.31999969482421875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 161 Loss : 0.518 \n",
      "Epoch : 161 Test Acc : 86.260\n",
      "Epoch : 161 Test Loss : 0.838 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.18000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 162 Loss : 0.507 \n",
      "Epoch : 162 Test Acc : 86.440\n",
      "Epoch : 162 Test Loss : 0.831 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.18000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 163 Loss : 0.515 \n",
      "Epoch : 163 Test Acc : 86.320\n",
      "Epoch : 163 Test Loss : 0.830 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.12000274658203125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 164 Loss : 0.505 \n",
      "Epoch : 164 Test Acc : 86.460\n",
      "Epoch : 164 Test Loss : 0.826 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.1399993896484375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 165 Loss : 0.512 \n",
      "Epoch : 165 Test Acc : 86.580\n",
      "Epoch : 165 Test Loss : 0.826 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.12000274658203125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 166 Loss : 0.500 \n",
      "Epoch : 166 Test Acc : 86.580\n",
      "Epoch : 166 Test Loss : 0.817 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 167 Loss : 0.499 \n",
      "Epoch : 167 Test Acc : 86.680\n",
      "Epoch : 167 Test Loss : 0.821 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = 0.09999847412109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 168 Loss : 0.494 \n",
      "Epoch : 168 Test Acc : 86.880\n",
      "Epoch : 168 Test Loss : 0.813 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.1999969482421875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 169 Loss : 0.499 \n",
      "Epoch : 169 Test Acc : 86.820\n",
      "Epoch : 169 Test Loss : 0.820 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.05999755859375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 170 Loss : 0.495 \n",
      "Epoch : 170 Test Acc : 86.580\n",
      "Epoch : 170 Test Loss : 0.809 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.23999786376953125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 171 Loss : 0.490 \n",
      "Epoch : 171 Test Acc : 86.680\n",
      "Epoch : 171 Test Loss : 0.806 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = 0.09999847412109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 172 Loss : 0.483 \n",
      "Epoch : 172 Test Acc : 86.780\n",
      "Epoch : 172 Test Loss : 0.805 \n",
      "Iterator for early stopping now at 4 since improvement of test accuracy = 0.09999847412109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 173 Loss : 0.482 \n",
      "Epoch : 173 Test Acc : 86.700\n",
      "Epoch : 173 Test Loss : 0.801 \n",
      "Iterator for early stopping now at 5 since improvement of test accuracy = -0.0800018310546875 <= 0.1\n",
      "Early stopping\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 174 Loss : 0.484 \n",
      "Epoch : 174 Test Acc : 86.600\n",
      "Epoch : 174 Test Loss : 0.796 \n",
      "Iterator for early stopping now at 6 since improvement of test accuracy = -0.09999847412109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 175 Loss : 0.488 \n",
      "Epoch : 175 Test Acc : 87.020\n",
      "Epoch : 175 Test Loss : 0.804 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.4199981689453125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 176 Loss : 0.480 \n",
      "Epoch : 176 Test Acc : 87.320\n",
      "Epoch : 176 Test Loss : 0.807 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.3000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 177 Loss : 0.481 \n",
      "Epoch : 177 Test Acc : 87.300\n",
      "Epoch : 177 Test Loss : 0.807 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.01999664306640625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 178 Loss : 0.476 \n",
      "Epoch : 178 Test Acc : 87.300\n",
      "Epoch : 178 Test Loss : 0.794 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 179 Loss : 0.474 \n",
      "Epoch : 179 Test Acc : 87.240\n",
      "Epoch : 179 Test Loss : 0.799 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = -0.06000518798828125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 180 Loss : 0.471 \n",
      "Epoch : 180 Test Acc : 87.540\n",
      "Epoch : 180 Test Loss : 0.792 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.3000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 181 Loss : 0.471 \n",
      "Epoch : 181 Test Acc : 87.000\n",
      "Epoch : 181 Test Loss : 0.790 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.5400009155273438 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 182 Loss : 0.469 \n",
      "Epoch : 182 Test Acc : 87.160\n",
      "Epoch : 182 Test Loss : 0.787 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.160003662109375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 183 Loss : 0.473 \n",
      "Epoch : 183 Test Acc : 87.480\n",
      "Epoch : 183 Test Loss : 0.791 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.31999969482421875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 184 Loss : 0.468 \n",
      "Epoch : 184 Test Acc : 87.320\n",
      "Epoch : 184 Test Loss : 0.790 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.160003662109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 185 Loss : 0.471 \n",
      "Epoch : 185 Test Acc : 87.280\n",
      "Epoch : 185 Test Loss : 0.789 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.04000091552734375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 186 Loss : 0.461 \n",
      "Epoch : 186 Test Acc : 87.480\n",
      "Epoch : 186 Test Loss : 0.784 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.20000457763671875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 187 Loss : 0.465 \n",
      "Epoch : 187 Test Acc : 87.580\n",
      "Epoch : 187 Test Loss : 0.779 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.09999847412109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 188 Loss : 0.459 \n",
      "Epoch : 188 Test Acc : 87.560\n",
      "Epoch : 188 Test Loss : 0.778 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.0200042724609375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 189 Loss : 0.454 \n",
      "Epoch : 189 Test Acc : 87.440\n",
      "Epoch : 189 Test Loss : 0.777 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = -0.1199951171875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 190 Loss : 0.454 \n",
      "Epoch : 190 Test Acc : 87.560\n",
      "Epoch : 190 Test Loss : 0.779 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.1199951171875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 191 Loss : 0.458 \n",
      "Epoch : 191 Test Acc : 87.560\n",
      "Epoch : 191 Test Loss : 0.772 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 192 Loss : 0.451 \n",
      "Epoch : 192 Test Acc : 87.560\n",
      "Epoch : 192 Test Loss : 0.775 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 193 Loss : 0.453 \n",
      "Epoch : 193 Test Acc : 87.540\n",
      "Epoch : 193 Test Loss : 0.774 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = -0.01999664306640625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 194 Loss : 0.451 \n",
      "Epoch : 194 Test Acc : 87.360\n",
      "Epoch : 194 Test Loss : 0.773 \n",
      "Iterator for early stopping now at 4 since improvement of test accuracy = -0.18000030517578125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 195 Loss : 0.454 \n",
      "Epoch : 195 Test Acc : 87.620\n",
      "Epoch : 195 Test Loss : 0.774 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.26000213623046875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 196 Loss : 0.448 \n",
      "Epoch : 196 Test Acc : 87.620\n",
      "Epoch : 196 Test Loss : 0.768 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 197 Loss : 0.445 \n",
      "Epoch : 197 Test Acc : 87.740\n",
      "Epoch : 197 Test Loss : 0.765 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.1199951171875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 198 Loss : 0.446 \n",
      "Epoch : 198 Test Acc : 87.840\n",
      "Epoch : 198 Test Loss : 0.770 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.09999847412109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 199 Loss : 0.439 \n",
      "Epoch : 199 Test Acc : 87.660\n",
      "Epoch : 199 Test Loss : 0.763 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.17999267578125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 200 Loss : 0.444 \n",
      "Epoch : 200 Test Acc : 87.600\n",
      "Epoch : 200 Test Loss : 0.762 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = -0.06000518798828125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 201 Loss : 0.439 \n",
      "Epoch : 201 Test Acc : 87.680\n",
      "Epoch : 201 Test Loss : 0.764 \n",
      "Iterator for early stopping now at 4 since improvement of test accuracy = 0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 202 Loss : 0.436 \n",
      "Epoch : 202 Test Acc : 87.800\n",
      "Epoch : 202 Test Loss : 0.759 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.12000274658203125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 203 Loss : 0.441 \n",
      "Epoch : 203 Test Acc : 87.740\n",
      "Epoch : 203 Test Loss : 0.762 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.06000518798828125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 204 Loss : 0.437 \n",
      "Epoch : 204 Test Acc : 87.780\n",
      "Epoch : 204 Test Loss : 0.758 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = 0.04000091552734375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 205 Loss : 0.433 \n",
      "Epoch : 205 Test Acc : 87.600\n",
      "Epoch : 205 Test Loss : 0.760 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = -0.18000030517578125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 206 Loss : 0.425 \n",
      "Epoch : 206 Test Acc : 87.880\n",
      "Epoch : 206 Test Loss : 0.753 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.279998779296875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 207 Loss : 0.434 \n",
      "Epoch : 207 Test Acc : 87.820\n",
      "Epoch : 207 Test Loss : 0.754 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.05999755859375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 208 Loss : 0.434 \n",
      "Epoch : 208 Test Acc : 87.820\n",
      "Epoch : 208 Test Loss : 0.754 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 209 Loss : 0.421 \n",
      "Epoch : 209 Test Acc : 88.120\n",
      "Epoch : 209 Test Loss : 0.748 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.3000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 210 Loss : 0.425 \n",
      "Epoch : 210 Test Acc : 88.080\n",
      "Epoch : 210 Test Loss : 0.742 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.04000091552734375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 211 Loss : 0.427 \n",
      "Epoch : 211 Test Acc : 87.980\n",
      "Epoch : 211 Test Loss : 0.747 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.09999847412109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 212 Loss : 0.420 \n",
      "Epoch : 212 Test Acc : 87.900\n",
      "Epoch : 212 Test Loss : 0.739 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = -0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 213 Loss : 0.418 \n",
      "Epoch : 213 Test Acc : 87.780\n",
      "Epoch : 213 Test Loss : 0.743 \n",
      "Iterator for early stopping now at 4 since improvement of test accuracy = -0.12000274658203125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 214 Loss : 0.418 \n",
      "Epoch : 214 Test Acc : 88.240\n",
      "Epoch : 214 Test Loss : 0.742 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.45999908447265625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 215 Loss : 0.423 \n",
      "Epoch : 215 Test Acc : 88.060\n",
      "Epoch : 215 Test Loss : 0.741 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.18000030517578125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 216 Loss : 0.420 \n",
      "Epoch : 216 Test Acc : 88.140\n",
      "Epoch : 216 Test Loss : 0.736 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = 0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 217 Loss : 0.412 \n",
      "Epoch : 217 Test Acc : 88.220\n",
      "Epoch : 217 Test Loss : 0.734 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = 0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 218 Loss : 0.413 \n",
      "Epoch : 218 Test Acc : 88.140\n",
      "Epoch : 218 Test Loss : 0.743 \n",
      "Iterator for early stopping now at 4 since improvement of test accuracy = -0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 219 Loss : 0.418 \n",
      "Epoch : 219 Test Acc : 88.040\n",
      "Epoch : 219 Test Loss : 0.741 \n",
      "Iterator for early stopping now at 5 since improvement of test accuracy = -0.09999847412109375 <= 0.1\n",
      "Early stopping\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 220 Loss : 0.412 \n",
      "Epoch : 220 Test Acc : 87.960\n",
      "Epoch : 220 Test Loss : 0.737 \n",
      "Iterator for early stopping now at 6 since improvement of test accuracy = -0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 221 Loss : 0.407 \n",
      "Epoch : 221 Test Acc : 88.060\n",
      "Epoch : 221 Test Loss : 0.732 \n",
      "Iterator for early stopping now at 7 since improvement of test accuracy = 0.09999847412109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 222 Loss : 0.413 \n",
      "Epoch : 222 Test Acc : 88.040\n",
      "Epoch : 222 Test Loss : 0.734 \n",
      "Iterator for early stopping now at 8 since improvement of test accuracy = -0.01999664306640625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 223 Loss : 0.408 \n",
      "Epoch : 223 Test Acc : 88.040\n",
      "Epoch : 223 Test Loss : 0.732 \n",
      "Iterator for early stopping now at 9 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 224 Loss : 0.413 \n",
      "Epoch : 224 Test Acc : 87.960\n",
      "Epoch : 224 Test Loss : 0.735 \n",
      "Iterator for early stopping now at 10 since improvement of test accuracy = -0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 225 Loss : 0.402 \n",
      "Epoch : 225 Test Acc : 88.200\n",
      "Epoch : 225 Test Loss : 0.726 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.23999786376953125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 226 Loss : 0.402 \n",
      "Epoch : 226 Test Acc : 88.200\n",
      "Epoch : 226 Test Loss : 0.727 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 227 Loss : 0.403 \n",
      "Epoch : 227 Test Acc : 88.520\n",
      "Epoch : 227 Test Loss : 0.729 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.31999969482421875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 228 Loss : 0.398 \n",
      "Epoch : 228 Test Acc : 88.400\n",
      "Epoch : 228 Test Loss : 0.722 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.1199951171875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 229 Loss : 0.400 \n",
      "Epoch : 229 Test Acc : 88.380\n",
      "Epoch : 229 Test Loss : 0.726 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.0200042724609375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 230 Loss : 0.400 \n",
      "Epoch : 230 Test Acc : 88.360\n",
      "Epoch : 230 Test Loss : 0.726 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = -0.01999664306640625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 231 Loss : 0.401 \n",
      "Epoch : 231 Test Acc : 88.360\n",
      "Epoch : 231 Test Loss : 0.726 \n",
      "Iterator for early stopping now at 4 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 232 Loss : 0.397 \n",
      "Epoch : 232 Test Acc : 88.260\n",
      "Epoch : 232 Test Loss : 0.725 \n",
      "Iterator for early stopping now at 5 since improvement of test accuracy = -0.09999847412109375 <= 0.1\n",
      "Early stopping\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 233 Loss : 0.395 \n",
      "Epoch : 233 Test Acc : 88.460\n",
      "Epoch : 233 Test Loss : 0.721 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.1999969482421875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 234 Loss : 0.395 \n",
      "Epoch : 234 Test Acc : 88.440\n",
      "Epoch : 234 Test Loss : 0.716 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.01999664306640625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 235 Loss : 0.397 \n",
      "Epoch : 235 Test Acc : 88.360\n",
      "Epoch : 235 Test Loss : 0.719 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 236 Loss : 0.390 \n",
      "Epoch : 236 Test Acc : 88.340\n",
      "Epoch : 236 Test Loss : 0.713 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = -0.0200042724609375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 237 Loss : 0.396 \n",
      "Epoch : 237 Test Acc : 88.460\n",
      "Epoch : 237 Test Loss : 0.723 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.12000274658203125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 238 Loss : 0.397 \n",
      "Epoch : 238 Test Acc : 88.220\n",
      "Epoch : 238 Test Loss : 0.727 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.23999786376953125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 239 Loss : 0.390 \n",
      "Epoch : 239 Test Acc : 88.200\n",
      "Epoch : 239 Test Loss : 0.709 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.0200042724609375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 240 Loss : 0.387 \n",
      "Epoch : 240 Test Acc : 88.500\n",
      "Epoch : 240 Test Loss : 0.711 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.3000030517578125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 241 Loss : 0.392 \n",
      "Epoch : 241 Test Acc : 88.620\n",
      "Epoch : 241 Test Loss : 0.713 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.12000274658203125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 242 Loss : 0.380 \n",
      "Epoch : 242 Test Acc : 88.600\n",
      "Epoch : 242 Test Loss : 0.705 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.0200042724609375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 243 Loss : 0.392 \n",
      "Epoch : 243 Test Acc : 88.860\n",
      "Epoch : 243 Test Loss : 0.709 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.26000213623046875 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 244 Loss : 0.384 \n",
      "Epoch : 244 Test Acc : 88.820\n",
      "Epoch : 244 Test Loss : 0.709 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.04000091552734375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 245 Loss : 0.385 \n",
      "Epoch : 245 Test Acc : 88.600\n",
      "Epoch : 245 Test Loss : 0.706 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.220001220703125 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 246 Loss : 0.391 \n",
      "Epoch : 246 Test Acc : 88.560\n",
      "Epoch : 246 Test Loss : 0.712 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = -0.04000091552734375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 247 Loss : 0.395 \n",
      "Epoch : 247 Test Acc : 88.680\n",
      "Epoch : 247 Test Loss : 0.717 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.12000274658203125 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 248 Loss : 0.386 \n",
      "Epoch : 248 Test Acc : 88.520\n",
      "Epoch : 248 Test Loss : 0.713 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.160003662109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 249 Loss : 0.385 \n",
      "Epoch : 249 Test Acc : 88.500\n",
      "Epoch : 249 Test Loss : 0.708 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.01999664306640625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 250 Loss : 0.378 \n",
      "Epoch : 250 Test Acc : 88.640\n",
      "Epoch : 250 Test Loss : 0.702 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.1399993896484375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 251 Loss : 0.380 \n",
      "Epoch : 251 Test Acc : 88.680\n",
      "Epoch : 251 Test Loss : 0.701 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.04000091552734375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 252 Loss : 0.378 \n",
      "Epoch : 252 Test Acc : 88.580\n",
      "Epoch : 252 Test Loss : 0.705 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.09999847412109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 253 Loss : 0.383 \n",
      "Epoch : 253 Test Acc : 88.500\n",
      "Epoch : 253 Test Loss : 0.698 \n",
      "Iterator for early stopping now at 3 since improvement of test accuracy = -0.0800018310546875 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 254 Loss : 0.376 \n",
      "Epoch : 254 Test Acc : 88.340\n",
      "Epoch : 254 Test Loss : 0.710 \n",
      "Iterator for early stopping now at 4 since improvement of test accuracy = -0.160003662109375 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 255 Loss : 0.375 \n",
      "Epoch : 255 Test Acc : 88.180\n",
      "Epoch : 255 Test Loss : 0.703 \n",
      "Iterator for early stopping now at 5 since improvement of test accuracy = -0.15999603271484375 <= 0.1\n",
      "Early stopping\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 256 Loss : 0.373 \n",
      "Epoch : 256 Test Acc : 88.660\n",
      "Epoch : 256 Test Loss : 0.703 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.48000335693359375 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 257 Loss : 0.372 \n",
      "Epoch : 257 Test Acc : 88.660\n",
      "Epoch : 257 Test Loss : 0.699 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = 0.0 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 258 Loss : 0.377 \n",
      "Epoch : 258 Test Acc : 88.560\n",
      "Epoch : 258 Test Loss : 0.701 \n",
      "Iterator for early stopping now at 2 since improvement of test accuracy = -0.100006103515625 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 259 Loss : 0.375 \n",
      "Epoch : 259 Test Acc : 88.800\n",
      "Epoch : 259 Test Loss : 0.701 \n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.2400054931640625 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Training time for 259 epochs: 18:18:54.935845\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAq8AAAI4CAYAAABEAgBmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzrDXxTZCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAI4CAYAAACY+3RRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAEON/AbtTWj8RmeTCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch : 0 Loss : 6.058 \n",
      "Epoch : 0 Test Acc : 19.080\n",
      "Epoch : 0 Test Loss : 2.194 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 19.079999923706055 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 1 Loss : 2.362 \n",
      "Epoch : 1 Test Acc : 21.180\n",
      "Epoch : 1 Test Loss : 2.232 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 2.1000003814697266 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 2 Loss : 2.067 \n",
      "Epoch : 2 Test Acc : 23.120\n",
      "Epoch : 2 Test Loss : 2.235 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.9400005340576172 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 3 Loss : 1.991 \n",
      "Epoch : 3 Test Acc : 24.460\n",
      "Epoch : 3 Test Loss : 2.230 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 1.3399982452392578 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 4 Loss : 1.958 \n",
      "Epoch : 4 Test Acc : 24.440\n",
      "Epoch : 4 Test Loss : 2.219 \n",
      "Iterator for early stopping now at 1 since improvement of test accuracy = -0.019998550415039062 <= 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 5 Loss : 1.940 \n",
      "Epoch : 5 Test Acc : 24.920\n",
      "Epoch : 5 Test Loss : 2.204 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.4799995422363281 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 6 Loss : 1.920 \n",
      "Epoch : 6 Test Acc : 25.700\n",
      "Epoch : 6 Test Loss : 2.183 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.7800006866455078 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 7 Loss : 1.911 \n",
      "Epoch : 7 Test Acc : 26.100\n",
      "Epoch : 7 Test Loss : 2.174 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.39999961853027344 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch : 8 Loss : 1.901 \n",
      "Epoch : 8 Test Acc : 26.720\n",
      "Epoch : 8 Test Loss : 2.162 \n",
      "UPDATE: NEW BEST MODEL\n",
      "Iterator for early stopping reset to 0 since improvement of test accuracy = 0.6199989318847656 > 0.1\n",
      "----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1c904d2eb0fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/perreaultlafleur/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-a56052b72160>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#print(self.conv(x).squeeze().reshape(64,-1).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m#return self.clf(self.conv(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/perreaultlafleur/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/perreaultlafleur/anaconda3/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/perreaultlafleur/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/perreaultlafleur/anaconda3/lib/python3.5/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m    140\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                             self.return_indices)\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/perreaultlafleur/anaconda3/lib/python3.5/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/perreaultlafleur/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[0mstride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     return torch.max_pool2d(\n\u001b[1;32m--> 488\u001b[1;33m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr_valid_acc=[]\n",
    "lr_train_loss=[]\n",
    "lr_valid_loss=[]\n",
    "\n",
    "for lr in np.arange(0.00005,0.00015,0.00002):\n",
    "\n",
    "    clf = Classifier()\n",
    "    optimizer = torch.optim.Adam(clf.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    epochs=260\n",
    "    time1=dt.datetime.now()\n",
    "\n",
    "    #Will print the plots\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    valid_acc = []\n",
    "    index_of_best = 0\n",
    "\n",
    "    #Early stopping criterion: when the test accuracy does not improve by at least #epsilon for #patience epochs in a row, we stop\n",
    "    patience = 5\n",
    "    epsilon = 0.1\n",
    "    patience_iterator = 0\n",
    "    best_test_acc = 0\n",
    "    last_acc = 0\n",
    "    stop = False\n",
    "\n",
    "    #Training for loop\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        # Train\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = clf(inputs.float())\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.data)\n",
    "            #print('|', end =\"\")\n",
    "\n",
    "        avg_train_losses.append(np.mean(losses))\n",
    "        print('\\nEpoch : %d Loss : %.3f ' % (epoch, np.mean(losses)))\n",
    "\n",
    "        # Evaluate\n",
    "        clf.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = clf(inputs.float())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "            loss = criterion(outputs, targets)\n",
    "            losses.append(loss.data)\n",
    "\n",
    "        avg_valid_losses.append(np.mean(losses))\n",
    "        test_acc = 100.*correct/total\n",
    "        valid_acc.append(test_acc)\n",
    "        print('Epoch : %d Test Acc : %.3f' % (epoch, test_acc))\n",
    "        print('Epoch : %d Test Loss : %.3f ' % (epoch, np.mean(losses)))\n",
    "\n",
    "        if test_acc > best_test_acc and not(stop):\n",
    "            print('UPDATE: NEW BEST MODEL')\n",
    "            filename = 'best_model_4_lr_%f.mdl' %lr\n",
    "            torch.save(clf.state_dict(), filename)\n",
    "            best_test_acc = test_acc\n",
    "            index_of_best = epoch\n",
    "        if test_acc <= last_acc + epsilon:\n",
    "            patience_iterator = patience_iterator + 1\n",
    "            print('Iterator for early stopping now at '+str(patience_iterator)+' since improvement of test accuracy = '+str((test_acc-last_acc).item())+' <= '+str(epsilon))\n",
    "        else:\n",
    "            patience_iterator = 0\n",
    "            print('Iterator for early stopping reset to 0 since improvement of test accuracy = '+str((test_acc-last_acc).item())+' > '+str(epsilon))\n",
    "        if patience_iterator == patience:\n",
    "            stop = True\n",
    "            print('Early stopping')\n",
    "\n",
    "        print('----------------------------------------------------------------------------------------------------------------------------')\n",
    "        last_acc = test_acc\n",
    "        clf.train()\n",
    "\n",
    "    #Training time\n",
    "    time2=dt.datetime.now()\n",
    "    print('Training time for '+str(epoch)+' epochs: '+str(time2-time1))\n",
    "    \n",
    "    # visualize the loss as the network trained\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    plt.plot(range(1,len(avg_train_losses)+1),avg_train_losses, label='Training Loss')\n",
    "    plt.plot(range(1,len(avg_valid_losses)+1),avg_valid_losses,label='Validation Loss')\n",
    "\n",
    "    # find position of lowest validation loss\n",
    "    minposs = index_of_best+1 \n",
    "    plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, 3.25) # consistent scale\n",
    "    plt.xlim(0, len(avg_train_losses)+1) # consistent scale\n",
    "    plt.title('Evolutions of the Training and Validation Cross-Entropy losses, and chosen model using Early Stopping')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # visualize the loss as the network trained\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    #plt.plot(range(1,len(avg_train_losses)+1),valid_acc, label='Training Loss')\n",
    "    plt.plot(range(1,len(valid_acc)+1),valid_acc,label='Validation Accuracy')\n",
    "\n",
    "    # find position of lowest validation loss\n",
    "    minposs = index_of_best+1 \n",
    "    plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 100) # consistent scale\n",
    "    plt.xlim(0, len(avg_train_losses)+1) # consistent scale\n",
    "    plt.title('Evolutions of the Training and Validation accuracies, and chosen model using Early Stopping ')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    lr_valid_acc.append(valid_acc)\n",
    "    lr_train_loss.append(avg_train_losses)\n",
    "    lr_valid_loss.append(avg_valid_losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize the loss as the network trained\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(avg_train_losses)+1),avg_train_losses, label='Training Loss')\n",
    "plt.plot(range(1,len(avg_valid_losses)+1),avg_valid_losses,label='Validation Loss')\n",
    "\n",
    "# find position of lowest validation loss\n",
    "minposs = index_of_best+1  \n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0, 3.25) # consistent scale\n",
    "plt.xlim(0, len(avg_train_losses)+1) # consistent scale\n",
    "plt.title('Evolutions of the Training and Validation Cross-Entropy losses, and chosen model using Early Stopping ')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# visualize the loss as the network trained\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "#plt.plot(range(1,len(avg_train_losses)+1),valid_acc, label='Training Loss')\n",
    "plt.plot(range(1,len(valid_acc)+1),valid_acc,label='Validation Accuracy')\n",
    "\n",
    "# find position of lowest validation loss\n",
    "minposs = index_of_best+1  \n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 100) # consistent scale\n",
    "plt.xlim(0, len(avg_train_losses)+1) # consistent scale\n",
    "plt.title('Evolutions of the Training and Validation accuracies, and chosen model using Early Stopping ')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE REAL PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): Dropout(p=0.5, inplace=False)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): Dropout(p=0.5, inplace=False)\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (clf): Linear(in_features=8192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Later to restore the best model\n",
    "# load the desired file ! clf.load_state_dict(torch.load('best_model.mdl'))\n",
    "clf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make the final predictions\n",
    "pred = []\n",
    "for batch_idx, inputs in enumerate(validloader):\n",
    "    inputs = Variable(inputs)\n",
    "    outputs = clf(inputs.float())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    pred.extend(predicted.tolist())\n",
    "final_preds = pd.DataFrame()\n",
    "final_preds['Id'] = list(range(0,10000))\n",
    "final_preds['Label'] = pred\n",
    "final_preds.to_csv('Prediction_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
